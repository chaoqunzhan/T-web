随着由ChatGPT引发的人工智能热潮，GPU成为了AI大模型训练平台的基石，甚至是决定性的算力底座。为什么GPU能力压CPU，成为炙手可热的主角呢？
要回答这个问题，首先需要了解当前人工智能（AI，Artificial Intelligence）的主要技术。
## 一、人工智能与深度学习

人工智能是一个历史非常长的学科。自上世纪50年代以来，在多个技术方向上进行过探索，也经历过多次的高潮和低谷。想了解更基础更有趣的人工智能介绍，可以看下面这篇文章：人工智能来了，小心你的饭碗不保。
人工智能在早期诞生了一个“不甚成功”的流派，叫做“人工神经网络”。这个技术的思路是，人脑的智慧无与伦比，要实现高级的人工智能，模仿人脑就是不二法门。
人脑是由数以亿计的神经元组成。这些神经元彼此连接，形成了庞大而复杂的神经网络。婴儿的大脑是一张白纸，经过后天的学习便可实现高度的智能。
参考人脑神经元，人工神经元模型就被设计了出来。
图片
在上图右侧的人工神经元里，通过调整每个输入的权重，经由神经元计算处理之后，便可得出相应的输出。这里面的每个权重，就被称作一个参数。
图片
把这样的多个神经元相互连接形成网络，就是人工神经网络了。人工神经网络一般由输入层、中间的多个隐藏层以及输出层组成。
这样的人工神经网络就像婴儿的大脑一样空空如也，必须给它投喂大量的数据，让它充分学习才能形成知识，才能用于实际解决问题。这个过程就叫做“深度学习”，属于“机器学习”的子集。
图片
以常见的“监督学习”为例，给AI投喂的数据必须包含问题和答案。比如说，我们的目标是让AI判断图片里面是不是有一只猫，那就需要给AI大量确定有猫的图片并给出猫的特征，让它自己从中找规律。
首先AI拿出一张给定的图片，采用初始权重得出自己的结论。然后比较这个结论和正确答案到底相差了多少，再返回去优化参数权重，这个过程循环进行，直至AI给出的结果和正确答案最为接近。
图片
这个学习的过程就叫做训练。一般来说，需要给AI大量含有正确答案的数据，才会得出比较好的训练结果。
一旦我们认为训练完成，就拿出试试成色。如果我们给它未知的问题，它也能很好地找出答案，就认为训练是成功的，AI的“泛化”效果很好。
图片
如上图所示，从神经网络上一层到下一层，参数权重的传递，本质上就是矩阵的乘法和加法。神经网络参数的规模越大，训练时需要的这些矩阵的计算也就越大。
最先进的深度学习神经网络可以有数百万到超过数万亿个参数，它们还需要大量的训练数据来实现高精度，这意味着必须通过正向和反向传递运行惊人的输入样本。由于神经网络是由大量相同的神经元创建的，因此这些计算本质上是高度并行的。
如此大规模的计算量，用CPU还是GPU好呢？
## 二、CPU，擅长控制的管家
我们先说CPU（Central Processing Unit）。
此物可谓电脑的大脑，是当仁不让的核心中的核心。
CPU内部主要包含运算器（也叫逻辑运算单元，ALU）和控制器（CU），以及一些寄存器和缓存。
图片

数据来了，会先放到存储器。然后，控制器会从存储器拿到相应数据，再交给运算器进行运算。运算完成后，再把结果返回到存储器。
在早期，一个CPU只有一套运算器、控制器和缓存，同一时间只能处理一个任务。要处理多个任务，只能按时间排队轮着来，大家雨露均沾。这样的CPU就是单核CPU。
图片
后来，人们把多套运算器、控制器和缓存集成在同一块芯片上，就组成了多核CPU。多核CPU拥有真正意义上的并行处理能力。
图片
一般情况下，多核CPU的核心数量少则2个4个，多则几十个。
在智能手机刚开始普及的时候，手机的外观趋同，其他地方也乏善可陈，厂家就大力渲染CPU的核数，史称智能手机的“核战”。
不过“核战”也就从双核烧到4核再到8核，然后大家也就都就偃旗息鼓了。芯片厂家也都是在这个核心数量上做优化。
为什么CPU不多集成一些核心呢？
这是因为CPU是一个通用处理器。它的任务非常复杂，既要应对不同类型的数据计算，还要响应人机交互。
复杂的任务管理和调度使得它需要更复杂的控制器和更大的缓存，进行逻辑控制和调度，保存各种任务状态，以降低任务切换时的时延。
CPU的核心越多，核心之间的互联通讯压力就越来越大，会降低单个核心的性能表现。并且，核心多了还会使功耗增加，如果忙闲不均，整体性能还可能不升反降。
## 三、GPU，并行计算专家

下来再看GPU（Graphics Processing Unit）。
GPU叫做图形处理单元。其设立的初衷是为了分担CPU的压力，加速三维图形的渲染，常用于电脑的显卡。
图像的处理，正是一种针对矩阵的密集并行计算。从下图可以看出，左侧的图像由大量的像素点组成，可以很自然地表示成右侧的矩阵。
图片
GPU一词从1999年Nvidia推出其GeForce256时开始流行，该产品对每一个像素点同时处理，执行图形转换、照明和三角剪裁等数学密集型并行计算，用于图像渲染。
为什么GPU善于承担密集的并行计算呢？这是因为GPU的在架构上和CPU有很大的不同。
图片
CPU的核数少，单个核心有足够多的缓存和足够强的运算能力，并辅助有很多加速分支判断甚至更复杂的逻辑判断的硬件，适合处理复杂的任务。
相比之下GPU就简单粗暴多了，每个核心的运算能力都不强，缓存也不大，就靠增加核心数量来提升整体能力。核心数量多了，就可以多管齐下，处理大量简单的并行计算工作。
图片

随着时间的推移，GPU也变得更加灵活和可编程，它的工作也就不局限于图像显示渲染了，还允许其他开发者用来加速高性能计算、深度学习等其他工作负载。
由于赶上了人工智能这样并行计算需求暴增的机遇，GPU一改以前的边缘角色，直接站到了舞台中央，可谓炙手可热。
GPU的名字，也变成了GPGPU，即通用GPU。
将AI训练这种并行性自然地映射到GPU，与仅使用 CPU 的训练相比，速度明显提升，并使它们成为训练大型、复杂的基于神经网络的系统的首选平台。推理操作的并行特性也非常适合在 GPU 上执行。
因此，由GPU作为主力所提供的算力，也被叫做“智算”。